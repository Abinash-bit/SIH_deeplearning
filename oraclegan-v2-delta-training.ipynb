{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **OracleGAN v2** - \n--------------\nThis study explores how to make **lighter, faster [OracleGAN](https://www.kaggle.com/lapl04/oraclegan-pix2pix-for-time-series-image)**.\n\nMobileNet v2 is adopted to Discriminator.\nAlso, part of Generator's convolution layers are replaced with Depthwise Separable Convolution.\nConvolution is added to Generator's skips.","metadata":{}},{"cell_type":"markdown","source":"# **Key Featues of OracleGAN**\n-------------------------------------\n- [Time Step Image Dataset](#time_step_image_dataset)\n- [Cost Function of Generator](#cost_of_generator)\n- [Cost Function of Discriminator](#cost_of_discriminator)","metadata":{}},{"cell_type":"markdown","source":"# **Key Featues of FastOracleGAN**\n-------------------------------------\n- [Depthwise Separable Convolution](#depthwise_separable_convolution)\n- [MobileNet v2](#mobilenetv2)\n- [Skip Convolution](#skip_convolution)","metadata":{}},{"cell_type":"markdown","source":"# **Key Featues of OracleGAN v2**\n-------------------------------------\n- ","metadata":{}},{"cell_type":"markdown","source":"# **Goal**\n- Predict future weather images using current weather images.","metadata":{}},{"cell_type":"markdown","source":"# Install additional libraries\n\nIQA_pytorch is a library which is used to calculate SSIM Score","metadata":{}},{"cell_type":"code","source":"!pip install IQA_pytorch #For SSIM Score","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:40:52.651683Z","iopub.execute_input":"2022-01-29T00:40:52.652187Z","iopub.status.idle":"2022-01-29T00:40:59.820924Z","shell.execute_reply.started":"2022-01-29T00:40:52.652149Z","shell.execute_reply":"2022-01-29T00:40:59.819945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchsummaryX","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:40:59.823002Z","iopub.execute_input":"2022-01-29T00:40:59.823371Z","iopub.status.idle":"2022-01-29T00:41:05.861276Z","shell.execute_reply.started":"2022-01-29T00:40:59.823332Z","shell.execute_reply":"2022-01-29T00:41:05.860305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pytorch_msssim","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:05.864196Z","iopub.execute_input":"2022-01-29T00:41:05.865819Z","iopub.status.idle":"2022-01-29T00:41:12.169258Z","shell.execute_reply.started":"2022-01-29T00:41:05.865764Z","shell.execute_reply":"2022-01-29T00:41:12.16824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torchvision\nfrom torch.optim import *\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport torch.nn.functional as F\nfrom IQA_pytorch import DISTS, utils\nfrom pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\nfrom torchsummaryX import summary\n\nimport math\nimport time\nimport numpy as np\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport albumentations\nimport albumentations.pytorch\nfrom matplotlib import pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import font_manager, rc\nfrom IPython import display\nimport random\nimport glob\nimport os\nfrom os import listdir\nfrom os.path import isfile, join\nimport warnings\nimport sys\nfrom tqdm import tqdm\nimport pickle\nimport gc\nimport random\nimport urllib.request\n\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Version of Torch : {0}\".format(torch.__version__))\nprint(\"Version of TorchVision : {0}\".format(torchvision.__version__))","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:12.17135Z","iopub.execute_input":"2022-01-29T00:41:12.171737Z","iopub.status.idle":"2022-01-29T00:41:12.184447Z","shell.execute_reply.started":"2022-01-29T00:41:12.171696Z","shell.execute_reply":"2022-01-29T00:41:12.183594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:12.185708Z","iopub.execute_input":"2022-01-29T00:41:12.186439Z","iopub.status.idle":"2022-01-29T00:41:12.345209Z","shell.execute_reply.started":"2022-01-29T00:41:12.186277Z","shell.execute_reply":"2022-01-29T00:41:12.344213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\nplt.rcParams['axes.unicode_minus'] = False\nfontpath = \"../input/koreanfont/NanumBrush.ttf\"\nfontprop = font_manager.FontProperties(fname=fontpath)\n\nplt.rcParams[\"animation.html\"] = \"jshtml\"\nplt.rcParams['figure.dpi'] = 150  \nplt.ioff()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:12.348756Z","iopub.execute_input":"2022-01-29T00:41:12.349275Z","iopub.status.idle":"2022-01-29T00:41:12.36041Z","shell.execute_reply.started":"2022-01-29T00:41:12.349226Z","shell.execute_reply":"2022-01-29T00:41:12.3595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Hyperparameters\n|Name of Hyperparameter|Explanation|\n|-----|-----|\n|USE_CUDA|whether to use GPU|\n|DEBUG|whether to print specific logs|\n|RANDOM_SEED|random seed of pytorch, random, numpy|\n|start_epoch|this is used to continuing train from checkpoint|\n|all_epochs|Epochs|\n|batch_size|Batch Size|\n|lrG|the learning rate of Generator|\n|lrD|the learning rate of Discriminator|\n|beta1, beta2|the beta1 and beta2 of Generator and Discriminator|\n|**L1Lambda**|lambda of pix2pix objective function|\n|**GAMMA**|factor similar to discount factor of DQN. (0<$\\gamma$<1) (check cost function of OracleGAN Generator)|\n|**INPUT_NUM**|the number of input images. (If INPUT_NUM is 1, this code works like FastOracleGAN of single input version)|\n|**TIME_STEP**|the number of future images which is used to calculate loss (check cost function of OracleGAN Generator)|\n\n","metadata":{}},{"cell_type":"code","source":"# Device\nUSE_CUDA = torch.cuda.is_available()\n\nprint(\"Device : {0}\".format(\"GPU\" if USE_CUDA else \"CPU\"))\ndevice = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\ncpu_device = torch.device(\"cpu\")\n\nDEBUG = False\n\nRANDOM_SEED = 2004\n\n# Train\nonly_d_train_step = 5\n\nstart_epoch_only_d = 0\nall_epochs_only_d = 0\nstart_epoch = 0\nall_epochs = 1\nbatch_size = 13\n\nlrG = 0.0002\nlrD = 0.0002\nbeta1 = 0.5\nbeta2 = 0.999\n\nL1lambda = 100\nSTART_GAMMA = 0\nIDLE_GAMMA = 500\nITER_GAMMA = 0.0008\nEND_GAMMA = 0.8\n\nINPUT_NUM = 4\nTIME_STEP = 4\nTEST_TIME_STEP = 6\nIMAGE_SIZE = 128\n\npatch = (1,256//2**4,256//2**4)\n\n# Path\nDATASET1_PATH = '../input/the-cloudcast-dataset'\n\n# Checkpoint\nUSE_CHECKPOINT = False\n\nOLD_PATH = '../input/fastoraclegan-multiple-input'\nOLD_GENERATOR_MODEL = os.path.join(OLD_PATH, 'Generator.pth')\nOLD_DISCRIMINATOR_MODEL = os.path.join(OLD_PATH, 'Discriminator.pth')\nOLD_G_LOSS = os.path.join(OLD_PATH, 'gloss.txt')\nOLD_D_LOSS = os.path.join(OLD_PATH, 'dloss.txt')","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:12.361789Z","iopub.execute_input":"2022-01-29T00:41:12.362324Z","iopub.status.idle":"2022-01-29T00:41:12.375761Z","shell.execute_reply.started":"2022-01-29T00:41:12.36228Z","shell.execute_reply":"2022-01-29T00:41:12.374664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"replay_memory = []\nGAMMA = START_GAMMA","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:12.377442Z","iopub.execute_input":"2022-01-29T00:41:12.37803Z","iopub.status.idle":"2022-01-29T00:41:12.38322Z","shell.execute_reply.started":"2022-01-29T00:41:12.37797Z","shell.execute_reply":"2022-01-29T00:41:12.38233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gamma_updater(now_gamma, now_iter, fixed_gamma=None):\n    if fixed_gamma != None:\n        return fixed_gamma\n    \n    gamma = now_gamma    \n    if now_iter >= IDLE_GAMMA:\n        gamma += ITER_GAMMA\n        if gamma >= END_GAMMA:\n            gamma = END_GAMMA\n        \n    return gamma","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:12.387112Z","iopub.execute_input":"2022-01-29T00:41:12.387704Z","iopub.status.idle":"2022-01-29T00:41:12.392995Z","shell.execute_reply.started":"2022-01-29T00:41:12.387665Z","shell.execute_reply":"2022-01-29T00:41:12.39215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def random_seed():\n    torch.manual_seed(RANDOM_SEED)\n    torch.cuda.manual_seed(RANDOM_SEED)\n    torch.cuda.manual_seed_all(RANDOM_SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(RANDOM_SEED)\n    random.seed(RANDOM_SEED)\n\n    print('Random Seed : {0}'.format(RANDOM_SEED))\n    \nrandom_seed()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:12.395054Z","iopub.execute_input":"2022-01-29T00:41:12.395644Z","iopub.status.idle":"2022-01-29T00:41:12.403533Z","shell.execute_reply.started":"2022-01-29T00:41:12.395578Z","shell.execute_reply":"2022-01-29T00:41:12.402489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def log(text):\n    global DEBUG\n    if DEBUG:\n        print(text)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:12.40515Z","iopub.execute_input":"2022-01-29T00:41:12.40591Z","iopub.status.idle":"2022-01-29T00:41:12.410556Z","shell.execute_reply.started":"2022-01-29T00:41:12.405865Z","shell.execute_reply":"2022-01-29T00:41:12.409681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize Data\n|Name of Function|Explanation|\n|-----|-----|\n|torch_tensor_to_plt|Convert torch image to matplotlib image|\n|plt_image_animation|show a video by update_function|","metadata":{}},{"cell_type":"code","source":"def torch_tensor_to_plt(img):\n    img = img.detach().numpy()[0]\n    img = np.transpose(img, (1, 2, 0))\n    return img ","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:12.412162Z","iopub.execute_input":"2022-01-29T00:41:12.413021Z","iopub.status.idle":"2022-01-29T00:41:12.41857Z","shell.execute_reply.started":"2022-01-29T00:41:12.412982Z","shell.execute_reply":"2022-01-29T00:41:12.417576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_video_in_jupyter_nb(width, height, video_url):\n    from IPython.display import HTML\n    return HTML(\"\"\"<video width=\"{}\" height=\"{}\" controls>\n    <source src={} type=\"video/mp4\">\n    </video>\"\"\".format(width, height, video_url))","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:12.420092Z","iopub.execute_input":"2022-01-29T00:41:12.420909Z","iopub.status.idle":"2022-01-29T00:41:12.426767Z","shell.execute_reply.started":"2022-01-29T00:41:12.420869Z","shell.execute_reply":"2022-01-29T00:41:12.425716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plt_image_animation(frames, update_func):\n    fig, ax = plt.subplots(figsize=(4,4))\n    plt.axis('off')\n    anim = animation.FuncAnimation(fig, update_func, frames=frames)\n    video = anim.to_html5_video()\n    html = display.HTML(video)\n    display.display(html)\n    plt.close()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:12.428352Z","iopub.execute_input":"2022-01-29T00:41:12.429174Z","iopub.status.idle":"2022-01-29T00:41:12.437673Z","shell.execute_reply.started":"2022-01-29T00:41:12.429138Z","shell.execute_reply":"2022-01-29T00:41:12.436716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt_image_animation(15, lambda t : plt.imshow(np.load(join(DATASET1_PATH, '2017M01', '{0}.npy'.format(t))), cmap='gray'))","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:12.439105Z","iopub.execute_input":"2022-01-29T00:41:12.439938Z","iopub.status.idle":"2022-01-29T00:41:22.3351Z","shell.execute_reply.started":"2022-01-29T00:41:12.4399Z","shell.execute_reply":"2022-01-29T00:41:22.334291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess Dataset","metadata":{}},{"cell_type":"code","source":"transformer = transforms.Compose([transforms.ToTensor(),\n                                  torchvision.transforms.Resize(IMAGE_SIZE),\n                                  transforms.Normalize((0.5), (0.5)), #GrayScale\n                                 ])\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:22.336818Z","iopub.execute_input":"2022-01-29T00:41:22.337584Z","iopub.status.idle":"2022-01-29T00:41:22.344102Z","shell.execute_reply.started":"2022-01-29T00:41:22.337533Z","shell.execute_reply":"2022-01-29T00:41:22.343335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"time_step_image_dataset\"></a>\n## **Time Step Image Dataset**\n\n------------------------------------------------\n\nOracleGAN calculates loss between predicted image and real image not only after 15 minutes but also **after 15×TimeStep minutes**.\n \nSo, dataset need to have **multiple output** images.","metadata":{}},{"cell_type":"code","source":"nowpath = \"\"\n\nclass TimeStepImageDataset(Dataset):\n    def __init__(self, date, input_num, time_step, transform=None):\n        self.date = date\n        self.input_num = input_num\n        self.time_step = time_step\n        self.transformer = transform\n        self.file = []\n        \n        file_list = glob.glob(join(self.date, '*'))\n        self.file = [file for file in file_list if (file.endswith(\".npy\") and not file.endswith('TIMESTAMPS.npy'))]\n        \n    def __len__(self):\n        return len(self.file)-self.time_step-self.input_num\n    \n    def transform(self, image):\n        if self.transformer:\n            return self.transformer(image)\n        else :\n            return image\n\n    def __getitem__(self, idx):\n        global nowpath\n        \n        log(join(self.date, str(idx)+'.npy'))\n        X_list = []\n        for i in range(0, self.input_num):\n            X_list.append(self.transform(np.load(join(self.date, str(idx+i)+'.npy'))).unsqueeze(0))\n        X = torch.cat(X_list)\n        nowpath = join(self.date, str(idx)+'.npy')\n\n        Y_list = []\n        for i in range(self.input_num, self.input_num+self.time_step):\n            Y_list.append(self.transform(np.load(join(self.date, str(idx+i)+'.npy'))).unsqueeze(0))\n        Y = torch.cat(Y_list)    \n\n        return X, Y","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:22.345354Z","iopub.execute_input":"2022-01-29T00:41:22.346004Z","iopub.status.idle":"2022-01-29T00:41:22.360373Z","shell.execute_reply.started":"2022-01-29T00:41:22.345962Z","shell.execute_reply":"2022-01-29T00:41:22.35957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATASET1_DIRS = glob.glob(join(DATASET1_PATH, '*'))\n\nrandom.shuffle(DATASET1_DIRS)\n\ntraindatasetlist = []\nfor ind, name in enumerate(DATASET1_DIRS[:20]):\n    traindatasetlist.append(TimeStepImageDataset(name, INPUT_NUM, TIME_STEP, transform=transformer))\ntrain_dataset = torch.utils.data.ConcatDataset(traindatasetlist)\n\ntestdatasetlist = []\nfor ind, name in enumerate(DATASET1_DIRS[20:]):\n    testdatasetlist.append(TimeStepImageDataset(name, INPUT_NUM, TEST_TIME_STEP, transform=transformer))\ntest_dataset = torch.utils.data.ConcatDataset(testdatasetlist)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:22.3618Z","iopub.execute_input":"2022-01-29T00:41:22.362409Z","iopub.status.idle":"2022-01-29T00:41:22.690688Z","shell.execute_reply.started":"2022-01-29T00:41:22.36237Z","shell.execute_reply":"2022-01-29T00:41:22.689864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\ntest_dataloader_bs1_shuffle = DataLoader(test_dataset, batch_size=1, shuffle=True) \ntest_dataloader_bs1_noshuffle = DataLoader(test_dataset, batch_size=1, shuffle=False) ","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:22.692265Z","iopub.execute_input":"2022-01-29T00:41:22.69264Z","iopub.status.idle":"2022-01-29T00:41:22.700552Z","shell.execute_reply.started":"2022-01-29T00:41:22.692587Z","shell.execute_reply":"2022-01-29T00:41:22.69971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ShowDatasetImage(x, y):\n    grid = torchvision.utils.make_grid(y)\n    \n    fig = plt.figure(figsize=(8, 2.5))\n    #plt.imshow(torch_tensor_to_plt(x.unsqueeze(0)), cmap='gray')\n    plt.axis('off')\n    plt.title('Input', fontproperties=fontprop)\n    for i in range(1, INPUT_NUM+1):\n        ax = fig.add_subplot(1, INPUT_NUM, i)\n        ax.axis('off')\n        ax.imshow(torch_tensor_to_plt(x[i-1].unsqueeze(0)), cmap='gray')\n        #ax.set_title('after {0} minutes'.format(15*i), fontproperties=fontprop)\n    plt.show()   \n\n    \n    fig = plt.figure(figsize=(8, 2.5))\n    plt.title('Real Weather Image', fontproperties=fontprop)\n    plt.axis('off')\n    for i in range(1, TIME_STEP+1):\n        ax = fig.add_subplot(1, TIME_STEP, i)\n        ax.axis('off')\n        ax.imshow(torch_tensor_to_plt(y[i-1].unsqueeze(0)), cmap='gray')\n        ax.set_title('after {0} minutes'.format(15*i), fontproperties=fontprop)\n    plt.show()\n\n    del x, y","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:22.702191Z","iopub.execute_input":"2022-01-29T00:41:22.702943Z","iopub.status.idle":"2022-01-29T00:41:22.714287Z","shell.execute_reply.started":"2022-01-29T00:41:22.702905Z","shell.execute_reply":"2022-01-29T00:41:22.713421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for ind, (x, y) in enumerate(train_dataset):\n    if ind != 0:\n        continue\n    ShowDatasetImage(x, y)\n    break","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:22.715883Z","iopub.execute_input":"2022-01-29T00:41:22.716663Z","iopub.status.idle":"2022-01-29T00:41:23.380124Z","shell.execute_reply.started":"2022-01-29T00:41:22.716601Z","shell.execute_reply":"2022-01-29T00:41:23.379218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Neural Networks and Optimizers\n|Name|Sort|\n|----|----|\n|Generator|UNet|\n|Discriminator|ResNet|\n|Optimizer of Generator|Adam|\n|Optimizer of Disciminator|Adam|","metadata":{}},{"cell_type":"markdown","source":"<a id=\"depthwise_separable_convolution\"></a>\n## Depthwise Separable Convolution\n------------------------\nGenerator's second~sixth convolution layers are replaced with Depthwise Separable Convolution.","metadata":{}},{"cell_type":"code","source":"class depthwise_separable_conv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):\n        super(depthwise_separable_conv, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=in_channels, bias=False)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n\n    def forward(self, x):\n        out = self.depthwise(x)\n        out = self.pointwise(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:23.381646Z","iopub.execute_input":"2022-01-29T00:41:23.381981Z","iopub.status.idle":"2022-01-29T00:41:23.389571Z","shell.execute_reply.started":"2022-01-29T00:41:23.381947Z","shell.execute_reply":"2022-01-29T00:41:23.388722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UNetDown(nn.Module):\n    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0, dsconv=True):\n        super().__init__()\n    \n        if dsconv:\n            layers = [depthwise_separable_conv(in_channels, out_channels, 4, stride=2, padding=1, bias=False)]\n        else :\n            layers = [nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False)]\n            \n        if normalize:\n            layers.append(nn.InstanceNorm2d(out_channels)),\n\n        layers.append(nn.LeakyReLU(0.2))\n\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.down = nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.down(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:23.390942Z","iopub.execute_input":"2022-01-29T00:41:23.39152Z","iopub.status.idle":"2022-01-29T00:41:23.400448Z","shell.execute_reply.started":"2022-01-29T00:41:23.391392Z","shell.execute_reply":"2022-01-29T00:41:23.399716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"skip_convolution\"></a>\n## Skip Convolution\n------------------------\n**1x1 Convolutions are added to Generator's Skip.**\n\nAt first, I tried adding **Attention Blocks**.\nBut, they are too **heavy** for the purpose of FastOracleGAN. Also, I thought the purpose to Generator is different to the original purpose of UNet.\nThe original purpose of UNet is Semantic Segmentation. Preventing to disturb input is important.\nHowever, the purpose of Generator is converting image. I estimated skip which adds original image to decoder layers can **interrupt** rather achieving the original purpose.\n**So I thought it was necessary to set Generator to be more interested in Up-sampling than Skip, instead of leaving it to artificial intelligence learning about which things to be interested in, such as Attention Blocks.** Therefore, I judged most of Generator's Attention Blocks can be substituted with just manipulating Skip with skip convolution.","metadata":{}},{"cell_type":"code","source":"class UNetUp(nn.Module):\n    def __init__(self, in_channels, out_channels, dropout=0.0, use_skip_conv=True):\n        super().__init__()\n\n        layers = [\n            nn.ConvTranspose2d(in_channels, out_channels,4,2,1,bias=False),\n            nn.InstanceNorm2d(out_channels),\n            nn.LeakyReLU()\n        ]\n\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.up = nn.Sequential(*layers)\n        self.use_skip_conv = use_skip_conv\n        \n        if use_skip_conv:\n            self.skip_conv = nn.Conv2d(out_channels, out_channels, 1, bias=False) #Skip Convolution\n\n    def forward(self,x,skip):\n        x = self.up(x)\n        if self.use_skip_conv:\n            skip = self.skip_conv(skip)\n        x = torch.cat((x, skip),1)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:23.401923Z","iopub.execute_input":"2022-01-29T00:41:23.402686Z","iopub.status.idle":"2022-01-29T00:41:23.412189Z","shell.execute_reply.started":"2022-01-29T00:41:23.402439Z","shell.execute_reply":"2022-01-29T00:41:23.411428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GeneratorUNet(nn.Module):\n    def __init__(self, in_channels=1, out_channels=1):\n        super().__init__()\n\n        self.down1 = UNetDown(in_channels, 64, normalize=False, dsconv=False)\n        self.down2 = UNetDown(64,128)                 \n        self.down3 = UNetDown(128,256)               \n        self.down4 = UNetDown(256,512,dropout=0.5) \n        self.down5 = UNetDown(512,512,dropout=0.5)      \n        self.down6 = UNetDown(512,512,dropout=0.5)             \n        #self.down7 = UNetDown(512,512,dropout=0.5)              \n        self.down8 = UNetDown(512,512,normalize=False,dropout=0.5, dsconv=False)\n\n        #self.up1 = UNetUp(512,512,dropout=0.5)\n        self.up2 = UNetUp(1024//2,512,dropout=0.5)\n        self.up3 = UNetUp(1024,512,dropout=0.5)\n        self.up4 = UNetUp(1024,512,dropout=0.5)\n        self.up5 = UNetUp(1024,256)\n        self.up6 = UNetUp(512,128)\n        self.up7 = UNetUp(256,64)\n        self.up8 = nn.Sequential(\n            nn.ConvTranspose2d(128,out_channels,4,stride=2,padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down8(d6)\n        u1 = d7\n        u2 = self.up2(u1,d6)\n        u3 = self.up3(u2,d5)\n        u4 = self.up4(u3,d4)\n        u5 = self.up5(u4,d3)\n        u6 = self.up6(u5,d2)\n        u7 = self.up7(u6,d1)\n        u8 = self.up8(u7)\n        \n        return u8","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:23.413521Z","iopub.execute_input":"2022-01-29T00:41:23.414304Z","iopub.status.idle":"2022-01-29T00:41:23.430131Z","shell.execute_reply.started":"2022-01-29T00:41:23.414143Z","shell.execute_reply":"2022-01-29T00:41:23.429316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"mobilenetv2\"></a>\n## MobileNet v2\n------------------------\nMobileNet v2 is adopted to Discriminator for better, lighter, and faster.","metadata":{}},{"cell_type":"code","source":"def _make_divisible(v, divisor, min_value=None):\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\ndef conv_3x3_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        assert stride in [1, 2]\n\n        hidden_dim = round(inp * expand_ratio)\n        self.identity = stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.identity:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, num_classes, patch=True, width_mult=1.):\n        super(Discriminator, self).__init__()\n        # setting of inverted residual blocks        \n        self.patch = patch\n        if self.patch:\n            self.cfgs = [\n                # t, c, n, s\n                [1,  16, 1, 1],\n                [6,  24, 2, 2],\n                [6,  32, 3, 2],\n            ]\n        elif self.patch:\n            self.cfgs = [\n                # t, c, n, s\n                [1,  16, 1, 1],\n                [6,  24, 2, 2],\n                [6,  32, 3, 2],\n                [6,  64, 4, 2],\n                [6,  96, 3, 1],\n                [6, 160, 3, 2],\n                [6, 320, 1, 1],\n            ]\n\n        # building first layer\n        input_channel = _make_divisible(32 * width_mult, 4 if width_mult == 0.1 else 8)\n        layers = [conv_3x3_bn(INPUT_NUM+1, input_channel, 2)]\n        # building inverted residual blocks\n        block = InvertedResidual\n        for t, c, n, s in self.cfgs:\n            output_channel = _make_divisible(c * width_mult, 4 if width_mult == 0.1 else 8)\n            for i in range(n):\n                layers.append(block(input_channel, output_channel, s if i == 0 else 1, t))\n                input_channel = output_channel\n        self.features = nn.Sequential(*layers)\n        # building last several layers\n        output_channel = _make_divisible(1280 * width_mult, 4 if width_mult == 0.1 else 8) if width_mult > 1.0 else 1280\n        self.conv = conv_1x1_bn(input_channel, output_channel)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        \n        self.d_classifier = nn.Linear(output_channel, 1)\n        self.frame_classifier = nn.Linear(output_channel, num_classes)\n        self.sigmoid = nn.Sigmoid()\n        self._initialize_weights()\n\n    def forward(self, x):\n        conv_feature = self.features(x)\n        conv_feature = self.conv(conv_feature)\n        \n        x = self.avgpool(conv_feature)\n        feature = x.view(x.size(0), -1)\n        dresult = self.d_classifier(feature) #real or fate\n        frame = self.frame_classifier(feature) #What time step?\n        #x = self.sigmoid(x)      \n             \n        if self.patch:\n            return (conv_feature, frame)\n        else:\n            return (dresult, frame)\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:23.431649Z","iopub.execute_input":"2022-01-29T00:41:23.432381Z","iopub.status.idle":"2022-01-29T00:41:23.469534Z","shell.execute_reply.started":"2022-01-29T00:41:23.432343Z","shell.execute_reply":"2022-01-29T00:41:23.468721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initiate Weights and Biases","metadata":{}},{"cell_type":"code","source":"def weights_init(m):\n    classname = m.__class__.__name__\n    if type(m) == nn.Conv2d:\n        m.weight.data.normal_(0.0, 0.02)\n    elif type(m) == nn.BatchNorm2d:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:23.474898Z","iopub.execute_input":"2022-01-29T00:41:23.47554Z","iopub.status.idle":"2022-01-29T00:41:23.481434Z","shell.execute_reply.started":"2022-01-29T00:41:23.475505Z","shell.execute_reply":"2022-01-29T00:41:23.480634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Generator = GeneratorUNet(in_channels=INPUT_NUM).to(device)\nDiscriminator = Discriminator(num_classes=TIME_STEP).to(device) \n\nsummary_g = Generator.apply(weights_init)\nsummary_d = Discriminator.apply(weights_init)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:23.483784Z","iopub.execute_input":"2022-01-29T00:41:23.484451Z","iopub.status.idle":"2022-01-29T00:41:23.808653Z","shell.execute_reply.started":"2022-01-29T00:41:23.484415Z","shell.execute_reply":"2022-01-29T00:41:23.80782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(Generator, torch.rand((batch_size, INPUT_NUM, IMAGE_SIZE, IMAGE_SIZE)).to(device))","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:23.810111Z","iopub.execute_input":"2022-01-29T00:41:23.810445Z","iopub.status.idle":"2022-01-29T00:41:23.951057Z","shell.execute_reply.started":"2022-01-29T00:41:23.81041Z","shell.execute_reply":"2022-01-29T00:41:23.950355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(Discriminator, torch.rand((batch_size, INPUT_NUM+1, IMAGE_SIZE, IMAGE_SIZE)).to(device))","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:23.952039Z","iopub.execute_input":"2022-01-29T00:41:23.952447Z","iopub.status.idle":"2022-01-29T00:41:24.101681Z","shell.execute_reply.started":"2022-01-29T00:41:23.952417Z","shell.execute_reply":"2022-01-29T00:41:24.100728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizerG = Adam(Generator.parameters(), lr=lrG, betas=(beta1, beta2))\noptimizerD = Adam(Discriminator.parameters(), lr=lrD, betas=(beta1, beta2))","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:24.103324Z","iopub.execute_input":"2022-01-29T00:41:24.103679Z","iopub.status.idle":"2022-01-29T00:41:24.111705Z","shell.execute_reply.started":"2022-01-29T00:41:24.103643Z","shell.execute_reply":"2022-01-29T00:41:24.110795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_list = []\nG_loss = []\nD_loss = []\n\nFAKE_LABEL = 0.0\nREAL_LABEL = 1.0","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:24.11328Z","iopub.execute_input":"2022-01-29T00:41:24.114084Z","iopub.status.idle":"2022-01-29T00:41:24.120473Z","shell.execute_reply.started":"2022-01-29T00:41:24.114046Z","shell.execute_reply":"2022-01-29T00:41:24.119565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Loss Functions","metadata":{}},{"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.logits = logits\n        self.reduce = reduce\n\n    def forward(self, inputs, targets):    \n        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n\n        pt = torch.exp(-ce_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:24.122027Z","iopub.execute_input":"2022-01-29T00:41:24.12276Z","iopub.status.idle":"2022-01-29T00:41:24.131984Z","shell.execute_reply.started":"2022-01-29T00:41:24.122722Z","shell.execute_reply":"2022-01-29T00:41:24.131041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l1loss = nn.L1Loss()\nl2loss = nn.MSELoss()\nsmoothl1loss = nn.SmoothL1Loss()\n\nbceloss = nn.BCEWithLogitsLoss() #nn.BCELoss()\nceloss = nn.CrossEntropyLoss()\nfocalloss = FocalLoss()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:24.133907Z","iopub.execute_input":"2022-01-29T00:41:24.134807Z","iopub.status.idle":"2022-01-29T00:41:24.144372Z","shell.execute_reply.started":"2022-01-29T00:41:24.134757Z","shell.execute_reply":"2022-01-29T00:41:24.143172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"cost_of_generator\"></a>\n## **Cost Function of Generator**\n\n------------------------------------------------\n\n**$$ \\mathbf{Loss_G(x, y) = \\sum_{i=1}^{t}\\gamma ^ {i-1}\\times \\left \\{ \\lambda _1 \\times  E_{x,y_i}\\left [ \\left \\| y_i - G^i(x) \\right \\|_1 \\right ] + E_{x}\\left [ log(1-D(G^i(x))) \\right ] \\right \\} } $$**\n\n$t$ is Time Step. $\\gamma$ is discount factor(GAMMA). $\\lambda _1$ is L1Lambda.","metadata":{}},{"cell_type":"code","source":"def generator_error(netG, netD, sketch, real, real_label, fake_label, gamma=0.0, b_size=batch_size):\n    def G_error(iter_input, G_output, real, D_output, D_target):\n        log(iter_input.shape)\n        log(G_output.shape)\n        log(real.shape)\n        return l1loss(G_output, real)*L1lambda + bceloss(D_output[0].view(-1), torch.tensor(REAL_LABEL).expand_as(D_output[0].view(-1)).to(device))\n    \n    next_input = sketch\n    error = None\n    \n    real_list = []\n    for i in range(TIME_STEP):\n        real_list.append(real[:,i,:,:,:])\n    \n    for ind, y in enumerate(real_list):\n        iter_input = next_input[:, -1, :, :].unsqueeze(1).clone().detach()\n        G_output = netG(next_input)\n        next_input = G_output.clone().detach()\n        if INPUT_NUM > 1:\n            next_input = torch.cat((sketch[:, 1:, :, :].clone().detach(), next_input), dim=1)\n        else :\n            next_input = G_output.clone().detach()\n        D_output = netD(torch.cat([sketch, G_output], dim=1))\n        \n        class_label = torch.full((b_size,), ind, dtype=torch.long, device=device)\n        \n        if ind==0:\n            error = G_error(iter_input, G_output, y, D_output, class_label)\n        else :\n            error += (gamma ** ind) * G_error(iter_input, G_output, y, D_output, class_label)\n            \n        del G_output, D_output\n        gc.collect()\n        torch.cuda.empty_cache()\n            \n    return error","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:24.146429Z","iopub.execute_input":"2022-01-29T00:41:24.147528Z","iopub.status.idle":"2022-01-29T00:41:24.163171Z","shell.execute_reply.started":"2022-01-29T00:41:24.147485Z","shell.execute_reply":"2022-01-29T00:41:24.162092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"cost_of_discriminator\"></a>\n## **Cost Function of Discriminator**\n\n------------------------------------------------\n\n**$$ \\mathbf{Loss_D(x, y) = E_x\\left [ log D(G(x)) \\right ] + \\frac{1}{t} \\sum_{i=1}^{t} E_{y_i}\\left [ log(1-D(G(y_i)))) \\right ]} $$**\n\n$t$ is Time Step.","metadata":{}},{"cell_type":"code","source":"def discriminator_error_only_d(netD, sketch, real, real_label, fake_label, b_size=batch_size, avg=True): \n    errD = 0.0    \n    for i in range(0, TIME_STEP):\n        outputs_real = netD(torch.cat([sketch, real[:,i,:,:,:]], dim=1))\n        \n        class_label = torch.full((b_size,), i, dtype=torch.long, device=device)\n        if avg:\n            errD += (l2loss(outputs_real[0].view(-1), torch.tensor(REAL_LABEL).expand_as(outputs_real[0].view(-1))).to(device))/TIME_STEP\n        else:\n            errD += focalloss(outputs_real[1], class_label) + bceloss(outputs_real[0].view(-1), real_label)\n        del outputs_real\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n    return errD","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:41:24.16531Z","iopub.execute_input":"2022-01-29T00:41:24.166377Z","iopub.status.idle":"2022-01-29T00:41:24.177814Z","shell.execute_reply.started":"2022-01-29T00:41:24.166336Z","shell.execute_reply":"2022-01-29T00:41:24.176531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def discriminator_error_in_gan(netG, netD, sketch, real, real_label, fake_label, b_size=batch_size, avg=True):\n    output_g = netG(sketch)\n    outputs_fake = netD(torch.cat([sketch, output_g.detach()], dim=1))    \n    log(outputs_fake[0].shape)\n    errD = bceloss(outputs_fake[0], torch.tensor(FAKE_LABEL).expand_as(outputs_fake[0]).to(device))\n    \n    del output_g, outputs_fake\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    for i in range(0, TIME_STEP):\n        outputs_real = netD(torch.cat([sketch, real[:,i,:,:,:]], dim=1))\n        \n        \n        class_label = torch.full((b_size,), i, dtype=torch.long, device=device)\n        if avg:\n            errD += l2loss(outputs_real[0].view(-1), torch.tensor(REAL_LABEL).expand_as(outputs_real[0].view(-1)).to(device))/TIME_STEP\n        else:\n            errD += focalloss(outputs_real[1], class_label) + bceloss(outputs_real[0].view(-1), real_label)\n        del outputs_real\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n    return errD","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:43:58.35168Z","iopub.execute_input":"2022-01-29T00:43:58.352023Z","iopub.status.idle":"2022-01-29T00:43:58.361529Z","shell.execute_reply.started":"2022-01-29T00:43:58.351993Z","shell.execute_reply":"2022-01-29T00:43:58.360644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Apply Checkpoint","metadata":{}},{"cell_type":"code","source":"def apply_checkpoint(use_checkpoint=True):\n    global Generator, Discriminator, optimizerG, optimizerD, G_Loss, D_Loss, start_epoch, all_epochs_only_d\n    \n    if os.path.isdir(OLD_PATH) and use_checkpoint:        \n        checkpoint = torch.load(OLD_GENERATOR_MODEL)\n        start_epoch = checkpoint['epoch']\n        Generator.load_state_dict(checkpoint['model_state_dict'])\n        optimizerG.load_state_dict(checkpoint['optimizer_state_dict'])\n        \n        checkpoint = torch.load(OLD_DISCRIMINATOR_MODEL)\n        start_epoch = checkpoint['epoch']\n        Discriminator.load_state_dict(checkpoint['model_state_dict'])\n        optimizerD.load_state_dict(checkpoint['optimizer_state_dict'])\n        \n        with open(OLD_G_LOSS, 'rb') as f:\n            G_loss = pickle.load(f)\n            \n        with open(OLD_D_LOSS, 'rb') as f:\n            D_loss = pickle.load(f)\n            \n        all_epochs_only_d = 0\n        \n        print('Continue training. (Epoch : {0})'.format(start_epoch))\n    else :\n        print('Begin training newly.')","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:43:58.653944Z","iopub.execute_input":"2022-01-29T00:43:58.654245Z","iopub.status.idle":"2022-01-29T00:43:58.662436Z","shell.execute_reply.started":"2022-01-29T00:43:58.654217Z","shell.execute_reply":"2022-01-29T00:43:58.661428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Train Function","metadata":{}},{"cell_type":"code","source":"nowepoch_only_d = 0\nnowepoch = 0","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:43:58.961541Z","iopub.execute_input":"2022-01-29T00:43:58.961892Z","iopub.status.idle":"2022-01-29T00:43:58.965925Z","shell.execute_reply.started":"2022-01-29T00:43:58.961862Z","shell.execute_reply":"2022-01-29T00:43:58.964737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Only Discriminator Train\n\ndef fit_only_d(device, num_epochs_only_d=1):\n    global nowepoch_only_d\n    iters = 0\n    for epoch in range(start_epoch_only_d+1, num_epochs_only_d+start_epoch_only_d+1):\n        nowepoch_only_d = epoch\n        print(\"< EPOCH{0} >\".format(epoch))\n        result = train_one_epoch_only_d(device, train_dataloader, Discriminator, optimizerD, epoch, num_epochs_only_d)\n        if not result:\n            return\n        \ndef train_one_epoch_only_d(device, dataloader, netD, optimizerD, epoch, num_epochs, iters=0):\n    global nowpath, strange_error_num, strange_error_limit\n    with torch.autograd.set_detect_anomaly(True):\n        for i, data in enumerate(dataloader):   \n            if i%only_d_train_step != 0 :\n                continue\n            start = time.time()\n            sketch, real = data\n            sketch, real = sketch.to(device), real.to(device)\n            \n            sketch_list = []\n            for ind in range(0, INPUT_NUM):\n                sketch_list.append(sketch[:, ind, :, :, :])\n            sketch = torch.cat(sketch_list, dim=1)\n            \n            b_size = sketch.size(0)\n            real_label = torch.full((b_size,), REAL_LABEL, dtype=torch.float, device=device)\n            fake_label = torch.full((b_size,), FAKE_LABEL, dtype=torch.float, device=device)\n\n            netD.train()\n            netD.zero_grad()\n            \n            errD = discriminator_error_only_d(netD, sketch, real, real_label, fake_label, b_size=b_size)\n            \n            log('Complete calcuating of Discriminator')\n            errD.backward()\n            log('Complete backprogration of Discriminator')\n            optimizerD.step()\n            log('Complete stepping OptimizerD')\n\n            \n            del b_size, real_label, fake_label, sketch, real\n            gc.collect()\n            torch.cuda.empty_cache()\n\n            #Log\n            if i % 1 == 0:\n                print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tTime: %.6f'\n                      % (epoch, num_epochs, i, len(dataloader),\n                         errD.item(), time.time() - start))\n                \n            D_loss.append(errD.item())\n            \n            del errD\n            gc.collect()\n            torch.cuda.empty_cache()\n\n            iters += 1\n    return True","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:43:59.117302Z","iopub.execute_input":"2022-01-29T00:43:59.117644Z","iopub.status.idle":"2022-01-29T00:43:59.130426Z","shell.execute_reply.started":"2022-01-29T00:43:59.117597Z","shell.execute_reply":"2022-01-29T00:43:59.129505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GAN Train\ndef fit_gan(device, num_epochs=1):\n    global nowepoch\n    gan_train_iters = 0\n    for epoch in range(start_epoch+1, num_epochs+start_epoch+1):\n        nowepoch = epoch\n        print(\"< EPOCH{0} >\".format(epoch))\n        gan_train_iters = train_one_epoch_gan(device, train_dataloader, Generator, Discriminator, optimizerG, optimizerD, epoch, num_epochs, iters=gan_train_iters)      \n    \n\ndef train_one_epoch_gan(device, dataloader, netG, netD, optimizerG, optimizerD, epoch, num_epochs, iters=0):\n    global nowpath, strange_error_num, strange_error_limit, replay_memory, GAMMA\n    with torch.autograd.set_detect_anomaly(True):\n        for i, data in enumerate(dataloader):   \n            start = time.time()\n            sketch, real = data\n            \n            sketch, real = sketch.to(device), real.to(device)\n            sketch_list = []\n            for ind in range(0, INPUT_NUM):\n                sketch_list.append(sketch[:, ind, :, :, :])\n            sketch = torch.cat(sketch_list, dim=1)           \n            \n            \n            b_size = sketch.size(0)\n            real_label = torch.full((b_size,), REAL_LABEL, dtype=torch.float, device=device)\n            fake_label = torch.full((b_size,), FAKE_LABEL, dtype=torch.float, device=device)\n            \n            #Train Discriminator\n            netG.eval()\n            netD.train()\n            netD.zero_grad()\n            \n            errD = discriminator_error_in_gan(netG, netD, sketch, real, real_label, fake_label, b_size=b_size)\n            \n            log('Complete calcuating of Discriminator')\n            errD.backward()\n            log('Complete backprogration of Discriminator')\n            optimizerD.step()\n            log('Complete stepping OptimizerD')\n        \n            #Train Generator\n            netG.train()\n            netD.eval()\n            netG.zero_grad()\n            \n            errG = generator_error(netG, netD, sketch, real, real_label, fake_label, gamma=GAMMA, b_size=b_size)\n            GAMMA = gamma_updater(GAMMA, iters)\n \n            log('Complete calcuating of Generator')\n            errG.backward()\n            log('Complete backprogration of Genereator')\n            optimizerG.step()\n            log('Complete stepping OptimizerG')\n            \n            del b_size, real_label, fake_label, sketch, real\n            gc.collect()\n            torch.cuda.empty_cache()\n\n            #Log\n            if i % 1 == 0:\n                print('[%d/%d][%d/%d]    Loss_G: %.4f  Loss_D: %.4f  Gamma: %f  Time: %.6f'\n                      % (epoch, num_epochs, i, len(dataloader),\n                         errG.item(), errD.item(), GAMMA, time.time() - start))\n                \n            \n\n            G_loss.append(errG.item())\n            D_loss.append(errD.item())\n            \n            del errG, errD\n            gc.collect()\n            torch.cuda.empty_cache()\n\n            iters += 1\n    return iters","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:43:59.266096Z","iopub.execute_input":"2022-01-29T00:43:59.266419Z","iopub.status.idle":"2022-01-29T00:43:59.280858Z","shell.execute_reply.started":"2022-01-29T00:43:59.266389Z","shell.execute_reply":"2022-01-29T00:43:59.279733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"def train_main():\n    global replay_memory, GAMMA\n    apply_checkpoint(use_checkpoint=USE_CHECKPOINT)\n    replay_memory = []\n    GAMMA = START_GAMMA\n    \n    # Only D\n    summary = Discriminator.train()\n    \n    if all_epochs_only_d>0:\n        print('-'*20)\n        print('Train Only D')\n        fit_only_d(device, num_epochs_only_d=all_epochs_only_d)\n        print('-'*20)\n\n    summary = Discriminator.eval()\n    \n    # G and D\n    summary = Generator.train()\n    summary = Discriminator.train()\n\n    if all_epochs>0:\n        print('-'*20)\n        print('Train G and D')\n        fit_gan(device, num_epochs=all_epochs)\n        print('-'*20)\n\n    summary = Generator.eval()\n    summary = Discriminator.eval()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:43:59.569335Z","iopub.execute_input":"2022-01-29T00:43:59.569602Z","iopub.status.idle":"2022-01-29T00:43:59.576126Z","shell.execute_reply.started":"2022-01-29T00:43:59.569575Z","shell.execute_reply":"2022-01-29T00:43:59.575074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_main()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:43:59.713326Z","iopub.execute_input":"2022-01-29T00:43:59.713595Z","iopub.status.idle":"2022-01-29T00:46:43.968109Z","shell.execute_reply.started":"2022-01-29T00:43:59.713571Z","shell.execute_reply":"2022-01-29T00:46:43.964798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test\n\n1. Calculate SSIM Score each Time Steps\n2. Generate test predicted images.\n3. Generate video which consist of series predicted images.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.title('Loss of Generator')\nplt.plot(G_loss,label=\"\")\nplt.xlabel(\"Iter\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:42:13.356864Z","iopub.status.idle":"2022-01-29T00:42:13.357895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.title('Loss of Discriminator')\nplt.plot(D_loss,label=\"train\")\nplt.xlabel(\"Iter\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:42:13.365573Z","iopub.status.idle":"2022-01-29T00:42:13.366583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_predict(model, time, input):\n    if time%15==0 and time!=0:\n        model.eval()\n        num = time//15\n        \n        final_answer = None\n        next_input = input\n        for i in range(num):\n            final_answer = model(next_input).clone().detach()\n            if INPUT_NUM > 1:\n                next_input = torch.cat((next_input[:, 1:, :, :], final_answer), dim=1)\n            else:\n                next_input = model(next_input).clone().detach()\n        return final_answer\n    else:\n        raise ValueError('Please set the time to a multiple of 15.')","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:42:13.368007Z","iopub.status.idle":"2022-01-29T00:42:13.369013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IQA_pytorch import SSIM, utils\n\ntoPILImage = transforms.ToPILImage()\nssim_model = SSIM(channels=1)\n\ndef one_time_step_ssim_score(dataloader, model, time_step, num=-1):\n    model.eval()\n    score = 0\n    total = 0\n    for ind, (x, y) in enumerate(test_dataloader_bs1_shuffle):\n        x, y = x.squeeze(0).to(device), y.squeeze(0).to(device)\n        x_list = []\n        for i in range(0, INPUT_NUM):\n            x_list.append(x[i, :, :, :])\n        x = torch.cat(x_list, dim=0).to(device)\n        outputG = model_predict(model, time_step*15, x.unsqueeze(0))\n\n        sketch = utils.prepare_image(toPILImage(outputG.squeeze(0))).to(device)\n        real = utils.prepare_image(toPILImage(y[time_step-1])).to(device)\n\n        score += ssim_model(sketch, real, as_loss=False).item()\n        total += 1\n\n        del x, y, outputG, sketch, real\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n        if num != -1:\n            if ind+1 >= num:\n                break\n            \n    print(\"SSIM Score of the prediction {0} minutes later : {1}\".format(time_step*15, score/total))\n    return score/total\n\nfor ind in range(1, TEST_TIME_STEP+1):\n    one_time_step_ssim_score(test_dataloader_bs1_shuffle, Generator, ind, num=2000)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:42:13.370449Z","iopub.status.idle":"2022-01-29T00:42:13.371488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **< SSIM Score of OracleGAN >**  *(check \"[OracleGAN - Pix2Pix for Time Series Image](https://www.kaggle.com/lapl04/oraclegan-pix2pix-for-time-series-image)\")*\n>\n> |Prediction|SSIM Score|\n> |-------------------|----------------|\n> |prediction 15 minutes later|0.8025623009409756|\n> |prediction 30 minutes later|0.7757316670715809|\n> |prediction 45 minutes later|0.7566662636697292|\n> |prediction 60 minutes later|0.7422156925499439|\n> |prediction 75 minutes later|0.7312239380329847|\n> |prediction 90 minutes later|0.720590250596404|","metadata":{}},{"cell_type":"markdown","source":"> **< SSIM Score of normal Pix2Pix >**  *(check \"[Pix2Pix (Compared to OracleNet)](https://www.kaggle.com/lapl04/pix2pix-compared-to-oraclenet)\")*\n>\n> |Prediction|SSIM Score|\n> |-------------------|----------------|\n> |prediction 15 minutes later|0.8199273004531861|\n> |prediction 30 minutes later|0.7006081487536431|\n> |prediction 45 minutes later|0.6030721757411956|\n> |prediction 60 minutes later|0.5150686911344529|","metadata":{}},{"cell_type":"code","source":"import zipfile\n\ny_nums = 40\niter = 0\n\nai_noseries_ls = []\nreal_noseries_ls = []\n\nstart_ind = 200\n\nfor ind, (x, y) in enumerate(test_dataloader_bs1_shuffle):\n    if ind < start_ind:\n        continue\n        \n    iter += 1\n    \n    x, y = x.to(device), y[0].to(device)\n    \n    x_list = []\n    for i in range(0, INPUT_NUM):\n        x_list.append(x[:, i, :, :, :])\n    x = torch.cat(x_list, dim=1).to(device)\n        \n    outputg = Generator(x).to(cpu_device)\n    \n    outputg = outputg*127.5+127.5\n    realimage = y*127.5+127.5\n\n    cv2.imwrite('./AI_NOSERIES_Answer{0}.png'.format(ind+1), torch_tensor_to_plt(outputg)*30)\n    cv2.imwrite('./Real_NOSERIES{0}.png'.format(ind+1), torch_tensor_to_plt(realimage.to(cpu_device))*30)\n    \n    ai_noseries_ls.append('./AI_NOSERIES_Answer{0}.png'.format(ind+1))\n    real_noseries_ls.append('./Real_NOSERIES{0}.png'.format(ind+1))\n    \n    if iter > y_nums:\n        break\n\nwith zipfile.ZipFile(\"ai_noseries.zip\", 'w') as my_zip:\n    for i in ai_noseries_ls:\n        my_zip.write(i)\n    my_zip.close()\n\n\nwith zipfile.ZipFile(\"real_noseries.zip\", 'w') as my_zip:\n    for i in real_noseries_ls:\n        my_zip.write(i)\n    my_zip.close()\n    \nfor file in (ai_noseries_ls + real_noseries_ls):\n    os.remove(file)\n    \nprint('NOSERIES Images are generated.')","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:42:13.372964Z","iopub.status.idle":"2022-01-29T00:42:13.373965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zipfile\n\ny_nums = 40\niter = 0\n\nai_series_ls = []\nreal_series_ls = []\n\nnext_input = None\nstart_ind = 200\n\nfor ind, (x, y) in enumerate(test_dataloader_bs1_noshuffle):\n    if ind < start_ind:\n        continue\n        \n    iter += 1\n    \n    x, y = x.to(device), y[0].to(device)\n    \n    x_list = []\n    for i in range(0, INPUT_NUM):\n        x_list.append(x[:, i, :, :, :])\n    x = torch.cat(x_list, dim=1).to(device)\n    \n    if ind == start_ind:\n        next_input = x.clone().detach().to(device)\n        #cv2.imwrite('./Input_SERIES.png', torch_tensor_to_plt(next_input[].to(cpu_device)*127.5+127.5)*30)\n    \n    outputg_series = Generator(next_input).to(cpu_device)\n    if INPUT_NUM > 1:\n        next_input = torch.cat((next_input[:, 1:, :, :].clone().detach(), outputg_series.clone().detach().to(device)), dim=1).to(device)\n    else:\n        next_input = outputg_series.clone().detach().to(device)\n        \n    outputg_series = outputg_series * 127.5 + 127.5\n    realimage = y*127.5+127.5\n\n    cv2.imwrite('./AI_SERIES_Answer{0}.png'.format(ind+1), torch_tensor_to_plt(outputg_series)*30)\n    cv2.imwrite('./Real_SERIES{0}.png'.format(ind+1), torch_tensor_to_plt(realimage.to(cpu_device))*30)\n    \n    ai_series_ls.append('./AI_SERIES_Answer{0}.png'.format(ind+1))\n    real_series_ls.append('./Real_SERIES{0}.png'.format(ind+1))\n    \n    if iter > y_nums:\n        break\n\nwith zipfile.ZipFile(\"ai_series.zip\", 'w') as my_zip:\n    for i in ai_series_ls:\n        my_zip.write(i)\n    my_zip.close()\n\nwith zipfile.ZipFile(\"real_series.zip\", 'w') as my_zip:\n    for i in real_series_ls:\n        my_zip.write(i)\n    my_zip.close()\n    \n\n    \nprint('SERIES Images are generated')","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:42:13.375467Z","iopub.status.idle":"2022-01-29T00:42:13.376524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v1 = cv2.VideoWriter('oraclegan_series.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 3, (128, 128))\nfor name in ai_series_ls:\n    img = cv2.imread(name)\n    v1.write(img)\nv1.release()\n\nv2 = cv2.VideoWriter('real_series.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 3, (128, 128))\nfor name in real_series_ls:\n    img = cv2.imread(name)\n    v2.write(img)\nv2.release()\n\nprint('Videos are generated')\nprint('video path : \"./oraclegan_series.mp4\" and \"./real_series.mp4\"')","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:42:13.378014Z","iopub.status.idle":"2022-01-29T00:42:13.379046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for file in (ai_series_ls + real_series_ls):\n    os.remove(file)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:42:13.380482Z","iopub.status.idle":"2022-01-29T00:42:13.381508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Checkpoint","metadata":{}},{"cell_type":"code","source":"torch.save({\n            'epoch': nowepoch,\n            'epoch_only_d': nowepoch_only_d,\n            'model_state_dict': Generator.state_dict(),\n            'optimizer_state_dict': optimizerG.state_dict(),\n            }, 'Generator.pth')\n\ntorch.save({\n            'epoch': nowepoch,\n            'epoch_only_d': nowepoch_only_d,\n            'model_state_dict': Discriminator.state_dict(),\n            'optimizer_state_dict': optimizerD.state_dict(),\n            }, 'Discriminator.pth')","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:42:13.38295Z","iopub.status.idle":"2022-01-29T00:42:13.383949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('./gloss.txt', 'wb') as f:\n    pickle.dump(G_loss, f)\nwith open('./dloss.txt', 'wb') as f:\n    pickle.dump(D_loss, f)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:42:13.385356Z","iopub.status.idle":"2022-01-29T00:42:13.386409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Conclusion**\n------------------\n**the time required of 1 iter training with FastOracleGAN(about 7.5s) is reduced more twice times than its OracleGAN(about 3.36s).**","metadata":{}}]}